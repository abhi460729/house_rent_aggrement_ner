{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "import os\n",
    "import docxpy\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import spacy \n",
    "  \n",
    "nlp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv(\"../data/TrainingTestSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique files in training dataframe: 55\n",
      "Total unique files in docx fmrat: 43\n"
     ]
    }
   ],
   "source": [
    "# find file which are present in training dataframe and folder (in docs format)\n",
    "present, absent = [], []\n",
    "for index, file_name in enumerate(tdf['File Name'].values):\n",
    "    file_path = fr\"../data/Training_data/{file_name}.pdf.docx\"\n",
    "    if os.path.exists(file_path):\n",
    "        present.append((index, file_path))\n",
    "    else:\n",
    "        absent.append(file_path)\n",
    "print(f\"Total unique files in training dataframe: {tdf['File Name'].nunique()}\")\n",
    "print(f\"Total unique files in docx fmrat: {len(set(present))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Aggrement Value</th>\n",
       "      <th>Aggrement Start Date</th>\n",
       "      <th>Aggrement End Date</th>\n",
       "      <th>Renewal Notice (Days)</th>\n",
       "      <th>Party One</th>\n",
       "      <th>Party Two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6683127-House-Rental-Contract-GERALDINE-GALINA...</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>20.05.2007</td>\n",
       "      <td>20.05.2008</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Antonio Levy S. Ingles, Jr. and/or Mary Rose C...</td>\n",
       "      <td>GERALDINE Q. GALINATO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6683129-House-Rental-Contract-Geraldine-Galina...</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>20.05.2007</td>\n",
       "      <td>20.05.2008</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Antonio Levy S. Ingles, Jr. and/or Mary Rose C...</td>\n",
       "      <td>GERALDINE Q. GALINATO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18325926-Rental-Agreement-1</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>05.12.2008</td>\n",
       "      <td>31.11.2009</td>\n",
       "      <td>90.0</td>\n",
       "      <td>MR.K.Kuttan</td>\n",
       "      <td>P.M. Narayana Namboodri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>36199312-Rental-Agreement</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>01.05.2010</td>\n",
       "      <td>31.04.2011</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Balaji.R</td>\n",
       "      <td>Kartheek R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44737744-Maddireddy-Bhargava-Reddy-Rental-Agre...</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>20.09.2010</td>\n",
       "      <td>19.07.2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M.V.V. VIJAYA SHANKAR</td>\n",
       "      <td>MADDIREDDY BHARGAVA REDDY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            File Name  Aggrement Value  \\\n",
       "8   6683127-House-Rental-Contract-GERALDINE-GALINA...           6500.0   \n",
       "9   6683129-House-Rental-Contract-Geraldine-Galina...           6500.0   \n",
       "10                        18325926-Rental-Agreement-1           4000.0   \n",
       "12                          36199312-Rental-Agreement           3800.0   \n",
       "13  44737744-Maddireddy-Bhargava-Reddy-Rental-Agre...           3000.0   \n",
       "\n",
       "   Aggrement Start Date Aggrement End Date  Renewal Notice (Days)  \\\n",
       "8            20.05.2007         20.05.2008                   15.0   \n",
       "9            20.05.2007         20.05.2008                   15.0   \n",
       "10           05.12.2008         31.11.2009                   90.0   \n",
       "12           01.05.2010         31.04.2011                   30.0   \n",
       "13           20.09.2010         19.07.2011                    NaN   \n",
       "\n",
       "                                            Party One  \\\n",
       "8   Antonio Levy S. Ingles, Jr. and/or Mary Rose C...   \n",
       "9   Antonio Levy S. Ingles, Jr. and/or Mary Rose C...   \n",
       "10                                       MR.K.Kuttan    \n",
       "12                                          Balaji.R    \n",
       "13                              M.V.V. VIJAYA SHANKAR   \n",
       "\n",
       "                    Party Two  \n",
       "8       GERALDINE Q. GALINATO  \n",
       "9       GERALDINE Q. GALINATO  \n",
       "10   P.M. Narayana Namboodri   \n",
       "12                 Kartheek R  \n",
       "13  MADDIREDDY BHARGAVA REDDY  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [i for i, _ in present]\n",
    "tdf.iloc[indices].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = docxpy.process(\"../data/Training_data/6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1.pdf.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## for all training data, extract and save the txt files\n",
    "# for present_file in present:\n",
    "#     text = docxpy.process(present_file[1])\n",
    "#     new_file_path = present_file[1].replace(\"Training_data\", \"Training_data_text\") + '.txt'\n",
    "#     with open(new_file_path, \"wb\") as f:\n",
    "#         f.write(text.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run NER on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "def print_all_entities(text_string):\n",
    "    doc = nlp(text_string) \n",
    "    for ent in doc.ents: \n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_) \n",
    "        \n",
    "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "print_all_entities(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000.0 --best match--> 01-09-2011\n",
      "01.09.2011 --best match--> 01-09-2011\n",
      "31.08.2012 --best match--> 01-09-2011\n",
      "nan --best match--> Tenant\n",
      "s parthasarathy --best match--> S Parthasarathy\n",
      "hari kiran tholeti --best match--> Hari Kiran Tholeti\n"
     ]
    }
   ],
   "source": [
    "# find most macthing phrase -- not working !!\n",
    "import difflib\n",
    "index = 14\n",
    "for x in tdf.iloc[present[index][0]].values[1:]:\n",
    "    x=str(x).lower()\n",
    "    best_match_score, best_match = 0, None\n",
    "    text = docxpy.process(present[index][1])\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        match_score = difflib.SequenceMatcher(None, ent.text.lower(), x).ratio()\n",
    "#         print(match_score)\n",
    "        if match_score > best_match_score:\n",
    "            best_match_score = match_score\n",
    "            best_match = ent\n",
    "    print(x, \"--best match-->\", best_match)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_all_entities(text)\n",
    "# difflib.SequenceMatcher(None, ent.text.lower()).ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read txt file and extract new annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/Training_data_text/100999172-House-Rental-Agreement.pdf.docx.txt\", \"r\") as f:\n",
    "#     text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from glob import glob\n",
    "\n",
    "new_labels = ['[start]', '[partyone]', '[partytwo]', '[rent]', '[end]', '[duration]']\n",
    "def clean_text(text_str):\n",
    "    for word in ['[start]', '[partyone]', '[partytwo]', '[rent]', '[end]', '[duration]', '{{', \"}}\"]:\n",
    "        text_str = text_str.replace(word, \"\")\n",
    "    return text_str\n",
    "\n",
    "def get_training_example(text):\n",
    "    training_example = []\n",
    "    offset = 0\n",
    "    for m in re.compile(\"{{.*?}}\\[.*?\\]\").finditer(text):\n",
    "    #     print(m.start(), m.group())\n",
    "        start = m.start() - offset\n",
    "        val = re.findall(\"{{.*?}}\", m.group())[0]\n",
    "        val_type = re.findall(\"\\[.*?\\]\", m.group())[0]\n",
    "        offset += 4 + 2 + len(val_type) - 2 \n",
    "        end = start + len(val) - 4\n",
    "        training_example.append((start, end, clean_text(val), val_type))\n",
    "    return training_example, clean_text(text)\n",
    "\n",
    "# run on all files, and only keep the ones which have training examples\n",
    "all_training_examples = []\n",
    "for file_path in glob(\"../data/Training_data_text/*\"):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        text = str(f.read(), \"utf-8\")\n",
    "    try:\n",
    "        example = get_training_example(text)\n",
    "    except Exception as e:\n",
    "        example = None\n",
    "    if example is not None:\n",
    "        all_training_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "15"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "all_training_examples = [x for x in all_training_examples if len(x[0])>0]\n",
    "len(all_training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to training format\n",
    "TRAIN_DATA = []\n",
    "for exs in all_training_examples:\n",
    "    entities = [(ex[0], ex[1], ex[3]) for ex in exs[0]]\n",
    "    TRAIN_DATA.append((exs[1], {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "15"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NER model\n",
    "\n",
    "source: https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-existing spacy model\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add the new label to ner\n",
    "for label in new_labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Losses {'ner': 9899.401126128156}\nLosses {'ner': 7544.85429251194}\nLosses {'ner': 7483.520290374756}\nLosses {'ner': 7121.713675275445}\nLosses {'ner': 7388.790061235428}\nLosses {'ner': 6944.072084188461}\nLosses {'ner': 7312.4352350234985}\nLosses {'ner': 7461.889449357986}\nLosses {'ner': 7356.691516757011}\nLosses {'ner': 7421.382605552673}\nLosses {'ner': 7160.302977561951}\nLosses {'ner': 7210.4822244644165}\nLosses {'ner': 7289.49045753479}\nLosses {'ner': 7094.23055934906}\nLosses {'ner': 7381.638561248779}\nLosses {'ner': 6797.321216583252}\nLosses {'ner': 7244.598215103149}\nLosses {'ner': 7048.762075424194}\nLosses {'ner': 7175.387539863586}\nLosses {'ner': 7185.857667922974}\n"
    }
   ],
   "source": [
    "# Importing requirements\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    # Training for 30 iterations     \n",
    "    for itn in range(200):\n",
    "        # shuffle examples before training\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "        # ictionary to store losses\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            # Calling update() over the iteration\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "        if itn%10==0:\n",
    "            print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "15"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "# TRAIN_DATA[1][0]\n",
    "len(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Entities [('Namashivayam', '[partyone]'), ('Mrs.', '[partyone]'), ('14500', '[rent]'), ('twelve months', '[duration]'), ('Jan 10, 2011.', '[start]'), ('Mrs.', '[partyone]')]\n"
    }
   ],
   "source": [
    "# Testing the model\n",
    "doc = nlp(TRAIN_DATA[0][0])\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "# print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in ['[start]', '[partyone]', '[partytwo]', '[rent]', '[end]', '[duration]', '{{', \"}}\"] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)\n",
    "\n",
    "# Save and load\n",
    "# Save the  model to directory\n",
    "# from pathlib import Path\n",
    "# output_dir = Path('../models/blank_learned_ner/')\n",
    "# nlp.to_disk(output_dir)\n",
    "# print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Load the saved model and predict\n",
    "# print(\"Loading from\", output_dir)\n",
    "# nlp_updated = spacy.load(output_dir)\n",
    "# doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
    "# print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training blank Spacy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NER from a blank spacy model\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe('ner'))\n",
    "\n",
    "nlp.begin_training()\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "    \n",
    "# Disable pipeline components you dont need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Losses {'ner': 5951.016310214996}\nLosses {'ner': 5989.326205253601}\nLosses {'ner': 6076.603830337524}\nLosses {'ner': 6092.696279525757}\nLosses {'ner': 6176.659318447113}\nLosses {'ner': 5905.225286483765}\nLosses {'ner': 6000.822365760803}\nLosses {'ner': 5973.484649658203}\nLosses {'ner': 5974.219406604767}\nLosses {'ner': 6037.844934463501}\nLosses {'ner': 6028.957887649536}\nLosses {'ner': 5945.0911955833435}\nLosses {'ner': 6037.5488867759705}\nLosses {'ner': 6018.058259963989}\nLosses {'ner': 5984.575963020325}\nLosses {'ner': 6007.937124252319}\nLosses {'ner': 5963.392322540283}\nLosses {'ner': 5950.990727424622}\nLosses {'ner': 5990.005153179169}\nLosses {'ner': 5999.3633670806885}\nLosses {'ner': 5989.132791519165}\nLosses {'ner': 6098.023772239685}\nLosses {'ner': 6000.967232704163}\nLosses {'ner': 6048.398564815521}\nLosses {'ner': 5942.407207489014}\nLosses {'ner': 6024.710395812988}\nLosses {'ner': 5993.76183795929}\nLosses {'ner': 6014.270380973816}\nLosses {'ner': 5859.240726470947}\nLosses {'ner': 6008.992926597595}\nLosses {'ner': 5995.492715835571}\nLosses {'ner': 5939.732089996338}\nLosses {'ner': 5920.062576293945}\nLosses {'ner': 5912.773990154266}\nLosses {'ner': 5893.675045967102}\nLosses {'ner': 5947.9497900009155}\nLosses {'ner': 5939.71648311615}\nLosses {'ner': 5980.006295204163}\nLosses {'ner': 6014.738409042358}\nLosses {'ner': 5988.843926906586}\n"
    }
   ],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(400):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "    if iteration%10==0:\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Entities [('M. Geetha', '[partyone]'), ('Siruvani Traders Private Limited', '[partytwo]'), ('Rs.2500', '[rent]'), ('11 Months', '[duration]')]\n"
    }
   ],
   "source": [
    "# Testing the model\n",
    "doc = nlp(TRAIN_DATA[2][0])\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "# print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in ['[start]', '[partyone]', '[partytwo]', '[rent]', '[end]', '[duration]', '{{', \"}}\"] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATA[6][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data and run model to generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result = []\n",
    "for file in glob(\"../data/Validation_Data/*\"):\n",
    "    # read and convert the doc to text\n",
    "    text = docxpy.process(file)\n",
    "    # extract entities\n",
    "    doc = nlp_updated(text)\n",
    "    result = {ent.label_:ent.text for ent in doc.ents}\n",
    "    result['file_name'] = file\n",
    "    # save them to df\n",
    "    validation_result.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                           file_name  \\\n0  ../data/Validation_Data\\156155545-Rental-Agree...   \n1  ../data/Validation_Data\\195231682-This-RENTAL-...   \n2  ../data/Validation_Data\\228094620-Rental-Agree...   \n3  ../data/Validation_Data\\239419594-Rental-Agree...   \n4  ../data/Validation_Data\\24158401-Rental-Agreem...   \n5  ../data/Validation_Data\\269135973-Udaya-Rental...   \n6  ../data/Validation_Data\\63793679-Rental-Agreem...   \n7  ../data/Validation_Data\\95980236-Rental-Agreem...   \n\n                                             [start]   [partyone]  \\\n0                                                NaN          NaN   \n1               06th day of March 2013 at Hyderabad.  C.BHAGYAMMA   \n2                                                NaN          NaN   \n3                                                NaN          NaN   \n4  1st day of April 2008 (1-04-08) by and between...          NaN   \n5                                                NaN          NaN   \n6  01-09-2011) at Bangalore by and between Mr. S ...          NaN   \n7                                                NaN          NaN   \n\n     [partytwo]       [duration]  \n0           NaN              NaN  \n1  JP INTERIO,”              NaN  \n2           NaN    eleven months  \n3           NaN        11 months  \n4           NaN  Twelve thousand  \n5           NaN              NaN  \n6           NaN    eleven months  \n7           NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>[start]</th>\n      <th>[partyone]</th>\n      <th>[partytwo]</th>\n      <th>[duration]</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../data/Validation_Data\\156155545-Rental-Agree...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../data/Validation_Data\\195231682-This-RENTAL-...</td>\n      <td>06th day of March 2013 at Hyderabad.</td>\n      <td>C.BHAGYAMMA</td>\n      <td>JP INTERIO,”</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../data/Validation_Data\\228094620-Rental-Agree...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>eleven months</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../data/Validation_Data\\239419594-Rental-Agree...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11 months</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../data/Validation_Data\\24158401-Rental-Agreem...</td>\n      <td>1st day of April 2008 (1-04-08) by and between...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Twelve thousand</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>../data/Validation_Data\\269135973-Udaya-Rental...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>../data/Validation_Data\\63793679-Rental-Agreem...</td>\n      <td>01-09-2011) at Bangalore by and between Mr. S ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>eleven months</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>../data/Validation_Data\\95980236-Rental-Agreem...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "pd.DataFrame(validation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap between training and test set -- we ignore these files while testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'195231682-This-RENTAL-AGREEMENT-is-Made-and-Executed-on-24th-Day-of-September.pdf.docx',\n",
       " '269135973-Udaya-Rental-Agreement.pdf.docx',\n",
       " '63793679-Rental-Agreement.pdf.docx',\n",
       " '95980236-Rental-Agreement.pdf.docx'}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfiles = [x[22:] for x in glob(\"../data/Training_data/*\")]\n",
    "vfiles = [x[24:] for x in glob(\"../data/Validation_Data/*\")]\n",
    "set(tfiles) & set(vfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['156155545-Rental-Agreement-Kns-Home.pdf.docx',\n",
       " '195231682-This-RENTAL-AGREEMENT-is-Made-and-Executed-on-24th-Day-of-September.pdf.docx',\n",
       " '228094620-Rental-Agreement.pdf.docx',\n",
       " '239419594-Rental-Agreement.pdf.docx',\n",
       " '24158401-Rental-Agreement.pdf.docx',\n",
       " '269135973-Udaya-Rental-Agreement.pdf.docx',\n",
       " '63793679-Rental-Agreement.pdf.docx',\n",
       " '95980236-Rental-Agreement.pdf.docx']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}